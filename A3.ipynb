{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Twitter geolocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use:\n",
    "1. extract data.zip to folder named \"/data\"\n",
    "2. simply run from start to end.\n",
    "3. For self-trained GloVe vectors, please use GloVe-master or clone the repo comes from git: https://github.com/stanfordnlp/GloVe.\n",
    "4. b1,b2,b5 are parameter setting in demo.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports for this assignment\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import ast\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing\n",
    "\n",
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data csv :['data/train_count.csv', 'data/train_full.csv', 'data/train_glove300.csv', 'data/train_tfidf.csv']\n",
      "dev data csv :['data/dev_count.csv', 'data/dev_full.csv', 'data/dev_glove300.csv', 'data/dev_tfidf.csv']\n",
      "test data csv :['data/test_count.csv', 'data/test_full.csv', 'data/test_glove300.csv', 'data/test_tfidf.csv']\n"
     ]
    }
   ],
   "source": [
    "# list files\n",
    "path = 'data/'\n",
    "files = os.listdir(path)\n",
    "# 'csv' files\n",
    "files_csv = [path+f for f in files if f[-4:] == '.csv']\n",
    "# train, dev and test set\n",
    "files_train = [f for f in files_csv if 'train' in f]\n",
    "files_dev = [f for f in files_csv if 'dev' in f]\n",
    "files_test = [f for f in files_csv if 'test' in f]\n",
    "\n",
    "print(f\"train data csv :{files_train}\")\n",
    "print(f\"dev data csv :{files_dev}\")\n",
    "print(f\"test data csv :{files_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full data set in dataframe\n",
    "train_full = [pd.read_csv(f,encoding ='utf-8',sep =',') for f in files_train if 'full' in f][0]\n",
    "dev_full = [pd.read_csv(f,encoding ='utf-8',sep =',') for f in files_dev if 'full' in f][0]\n",
    "test_full = [pd.read_csv(f,encoding ='utf-8',sep =',') for f in files_test if 'full' in f][0]\n",
    "# tfidf data set in dataframe\n",
    "train_count = [pd.read_csv(f,encoding ='utf-8',sep =',') for f in files_train if 'count' in f][0]\n",
    "dev_count = [pd.read_csv(f,encoding ='utf-8',sep =',') for f in files_dev if 'count' in f][0]\n",
    "test_count = [pd.read_csv(f,encoding ='utf-8',sep =',') for f in files_test if 'count' in f][0]\n",
    "# tfidf data set in dataframe\n",
    "train_tfidf = [pd.read_csv(f,encoding ='utf-8',sep =',') for f in files_train if 'tfidf' in f][0]\n",
    "dev_tfidf = [pd.read_csv(f,encoding ='utf-8',sep =',') for f in files_dev if 'tfidf' in f][0]\n",
    "test_tfidf = [pd.read_csv(f,encoding ='utf-8',sep =',') for f in files_test if 'tfidf' in f][0]\n",
    "# glove300 data set in dataframe\n",
    "train_glove = [pd.read_csv(f,encoding ='utf-8',sep =',') for f in files_train if 'glove' in f][0]\n",
    "dev_glove = [pd.read_csv(f,encoding ='utf-8',sep =',') for f in files_dev if 'glove' in f][0]\n",
    "test_glove = [pd.read_csv(f,encoding ='utf-8',sep =',') for f in files_test if 'glove' in f][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain a list of vocab\n",
    "vocab_file = open(\"data/vocab.txt\",'r').readlines()\n",
    "vocab = dict()\n",
    "for line in vocab_file:\n",
    "    vocab.update({line.split(\"\\t\")[1].replace(\"\\n\",\"\"):line.split(\"\\t\")[0]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>133795</td>\n",
       "      <td>133795</td>\n",
       "      <td>133795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "      <td>3400</td>\n",
       "      <td>119102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NORTHEAST</td>\n",
       "      <td>USER_6b07169e</td>\n",
       "      <td>[(0, 1.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>52582</td>\n",
       "      <td>242</td>\n",
       "      <td>877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           region           user       tweet\n",
       "count      133795         133795      133795\n",
       "unique          4           3400      119102\n",
       "top     NORTHEAST  USER_6b07169e  [(0, 1.0)]\n",
       "freq        52582            242         877"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11475</td>\n",
       "      <td>11475</td>\n",
       "      <td>11475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>10777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NORTHEAST</td>\n",
       "      <td>USER_34d2b648</td>\n",
       "      <td>[(0, 1.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4295</td>\n",
       "      <td>139</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           region           user       tweet\n",
       "count       11475          11475       11475\n",
       "unique          4            300       10777\n",
       "top     NORTHEAST  USER_34d2b648  [(0, 1.0)]\n",
       "freq         4295            139          77"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_tfidf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into sets\n",
    "\n",
    "Note: test_{set}_Y is basically nothing, it will be the output dataframe then transformed as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full\n",
    "train_full_X = train_full.iloc[:,1:]\n",
    "train_full_Y = train_full.iloc[:,:1]\n",
    "dev_full_X = dev_full.iloc[:,1:]\n",
    "dev_full_Y = dev_full.iloc[:,:1]\n",
    "test_full_X = test_full.iloc[:,1:]\n",
    "test_full_Y = test_full.iloc[:,:1]\n",
    "# count\n",
    "train_count_X = train_count.iloc[:,1:]\n",
    "train_count_Y = train_count.iloc[:,:1]\n",
    "dev_count_X = dev_count.iloc[:,1:]\n",
    "dev_count_Y = dev_count.iloc[:,:1]\n",
    "test_count_X = test_count.iloc[:,1:]\n",
    "test_count_Y = test_count.iloc[:,:1]\n",
    "# tfidf\n",
    "train_tfidf_X = train_tfidf.iloc[:,1:]\n",
    "train_tfidf_Y = train_tfidf.iloc[:,:1]\n",
    "dev_tfidf_X = dev_tfidf.iloc[:,1:]\n",
    "dev_tfidf_Y = dev_tfidf.iloc[:,:1]\n",
    "test_tfidf_X = test_tfidf.iloc[:,1:]\n",
    "test_tfidf_Y = test_tfidf.iloc[:,:1]\n",
    "# glove\n",
    "train_glove_X = train_glove.iloc[:,1:]\n",
    "train_glove_Y = train_glove.iloc[:,:1]\n",
    "dev_glove_X = dev_glove.iloc[:,1:]\n",
    "dev_glove_Y = dev_glove.iloc[:,:1]\n",
    "test_glove_X = test_glove.iloc[:,1:]\n",
    "test_glove_Y = test_glove.iloc[:,:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiment 1: Baseline \n",
    "\n",
    "In this case, I decided to use the most basic way to classify a label.\n",
    "#### How: \n",
    "implement a 1r classifier for whether it has the most frequent word or not. e.g. NORTHEAST has wordID=0 for hishgest frequency, so that any record has this word will be classified as NORTHEAST\n",
    "\n",
    "## Get highest frequency word for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = train_count_X['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[\"NORTHEAST\",\"MIDWEST\",\"WEST\",\"SOUTH\"]\n",
    "label_records=dict() # label_records := {\"NORTHEAST\":[tweet_has_NORTHEAST_label_1, tweet_has_NORTHEAST_label_2,...], \"MIDWEST\":[...],...}\n",
    "for label in labels:\n",
    "    label_records.update({label:[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_count_Y['region'])):\n",
    "    if train_count_Y['region'][i] in label_records.keys():\n",
    "        label_records[train_count_Y['region'][i]].append(tweets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NORTHEAST': {}, 'MIDWEST': {}, 'WEST': {}, 'SOUTH': {}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_word_count = dict().fromkeys(labels,{})\n",
    "labeled_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max word found: 0, name:! , for label: NORTHEAST, with freq: 23719/368404\n",
      "2nd max word found: 69, name:. , for label: NORTHEAST, with freq: 12459/368404\n",
      "3rd max word found: 1464, name:rt , for label: NORTHEAST, with freq: 12141/368404\n",
      "----------------------\n",
      "max word found: 0, name:! , for label: MIDWEST, with freq: 9918/112727\n",
      "2nd max word found: 69, name:. , for label: MIDWEST, with freq: 5526/112727\n",
      "3rd max word found: 140, name:? , for label: MIDWEST, with freq: 3292/112727\n",
      "----------------------\n",
      "max word found: 0, name:! , for label: WEST, with freq: 9125/116191\n",
      "2nd max word found: 69, name:. , for label: WEST, with freq: 5834/116191\n",
      "3rd max word found: 140, name:? , for label: WEST, with freq: 3567/116191\n",
      "----------------------\n",
      "max word found: 0, name:! , for label: SOUTH, with freq: 32098/366552\n",
      "2nd max word found: 69, name:. , for label: SOUTH, with freq: 15082/366552\n",
      "3rd max word found: 73, name:... , for label: SOUTH, with freq: 10750/366552\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    word_count = vocab.fromkeys(vocab, 0)\n",
    "    for tweet in label_records[label]:\n",
    "        t = ast.literal_eval(tweet)\n",
    "        for word in t:\n",
    "            word_count[str(word[0])] += int(word[1])\n",
    "    sorted_count =dict(sorted(word_count.items(), key=lambda item: item[1],reverse=True))\n",
    "    labeled_word_count[label] = sorted_count\n",
    "    it = iter(sorted_count.keys())\n",
    "    max_word,max_word_2nd,max_word_3rd=next(it),next(it),next(it)\n",
    "    print(f\"max word found: {max_word}, name:{vocab[max_word]} , for label: {label}, with freq: {word_count[max_word]}/{sum(word_count.values())}\")\n",
    "    # For observation only:\n",
    "    print(f\"2nd max word found: {max_word_2nd}, name:{vocab[max_word_2nd]} , for label: {label}, with freq: {word_count[max_word_2nd]}/{sum(word_count.values())}\")\n",
    "    print(f\"3rd max word found: {max_word_3rd}, name:{vocab[max_word_3rd]} , for label: {label}, with freq: {word_count[max_word_3rd]}/{sum(word_count.values())}\")\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above results, we can see that the high frequency words are mostly meaningless as exclamation mark is always the most frequent one, and according to the reverse-sorted dictionary that first 3 high frequency terms are punctuation. Only one character combination is involed in the frequency ranking.\n",
    "## Predict based on high frequency term.\n",
    "\n",
    "In this case, since all 4 labels are having same most high frequency term \"!\", thus this one-R classfier becomes a random label distribution since all labels has same most 'important' term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.24871459694989106\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "tweet_dev_X = dev_count_X['tweet']\n",
    "tweet_dev_Y = dev_count_Y\n",
    "tweet_test_X = test_count_X['tweet']\n",
    "tweet_test_Y = test_count_Y\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for tweet in tweet_dev_X:\n",
    "    predictions.append(random.choice(labels))\n",
    "total = len(tweet_dev_X)\n",
    "hit = 0\n",
    "for i in range(total):\n",
    "    if tweet_dev_Y['region'][i] == predictions[i]:\n",
    "        hit+=1\n",
    "print(\"accuracy:\",hit/total)\n",
    "# For test data\n",
    "#for i in range(len(tweet_test_Y)):\n",
    "#    tweet_test_Y.loc[i,'region'] = random.choice(labels)\n",
    "#    tweet_test_Y.loc[i,'id'] = int(i+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For output only:\n",
    "# columns_titles = [\"id\",\"region\"]\n",
    "# tweet_test_Y=tweet_test_Y.reindex(columns=columns_titles)\n",
    "# tweet_test_Y.id.astype(int)\n",
    "# tweet_test_Y.to_csv('0r.csv',index=False,float_format='%.f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiment 2: Linear models (NB and LR) with glove 300d\n",
    "\n",
    "\n",
    "#### How: \n",
    "Use Glove300 for input of Naive Bayes. Since linear models performs well in high-dimensional data\n",
    "\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 300 0's string\n",
    "zeros_300 = [str(0)] * 300\n",
    "zeros_300 = \" \".join(zeros_300)\n",
    "\n",
    "# Since there are 0's in the instance, which should not be used during model training.\n",
    "train_glove_ignored= train_glove[train_glove['tweet'] == zeros_300]\n",
    "train_glove_trim = train_glove[train_glove['tweet'] != zeros_300]\n",
    "dev_glove_ignored = dev_glove[dev_glove['tweet']==zeros_300]\n",
    "dev_glove_trim = dev_glove[dev_glove['tweet']!=zeros_300]\n",
    "# The test data should remain the same, below code is used only for statistics. (see the print statement next block)\n",
    "test_glove_ignored = test_glove[test_glove['tweet']==zeros_300]\n",
    "test_glove_trim = test_glove[test_glove['tweet']!=zeros_300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 133795 training data in total, with 1442 invalid instances, and 132353 valid instances\n",
      "there are 11475 dev data in total, with 100 invalid instances, and 11375 valid instances\n",
      "there are 12018 test data in total, with 137 invalid instances, and 11881 valid instances\n"
     ]
    }
   ],
   "source": [
    "print(f\"there are {train_glove['tweet'].count()} training data in total, with {train_glove_ignored['tweet'].count()} invalid instances, and {train_glove_trim['tweet'].count()} valid instances\")\n",
    "print(f\"there are {dev_glove['tweet'].count()} dev data in total, with {dev_glove_ignored['tweet'].count()} invalid instances, and {dev_glove_trim['tweet'].count()} valid instances\")\n",
    "print(f\"there are {test_glove['tweet'].count()} test data in total, with {test_glove_ignored['tweet'].count()} invalid instances, and {test_glove_trim['tweet'].count()} valid instances\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the glove 300 has relatively large data size. Thus it will be randomly split into 3 sets.\n",
    "shuffled = train_glove_trim.sample(frac=1)\n",
    "train_glove_split = np.array_split(shuffled, 3)\n",
    "\n",
    "# for simplicity, assign them with separate names.\n",
    "train_glove_X = train_glove_trim.iloc[:,1:]\n",
    "train_glove_X_1 = train_glove_split[0].iloc[:,1:]\n",
    "train_glove_X_2 = train_glove_split[1].iloc[:,1:]\n",
    "train_glove_X_3 = train_glove_split[2].iloc[:,1:]\n",
    "train_glove_X_ignored = train_glove_ignored.iloc[:,1:]\n",
    "\n",
    "train_glove_Y = train_glove_trim.iloc[:,:1]\n",
    "train_glove_Y_1 = train_glove_split[0].iloc[:,:1]\n",
    "train_glove_Y_2 = train_glove_split[1].iloc[:,:1]\n",
    "train_glove_Y_3 = train_glove_split[2].iloc[:,:1]\n",
    "train_glove_Y_ignored = train_glove_ignored.iloc[:,:1]\n",
    "\n",
    "# for each training set, transform data from a giant string to multiple columns, each column means 1 dimension.\n",
    "train_glove_X=train_glove_X['tweet'].str.split(\" \",expand=True)\n",
    "train_glove_X_1t=train_glove_X_1['tweet'].str.split(\" \",expand=True)\n",
    "train_glove_X_2t=train_glove_X_2['tweet'].str.split(\" \",expand=True)\n",
    "train_glove_X_3t=train_glove_X_3['tweet'].str.split(\" \",expand=True)\n",
    "train_glove_X_ignored = train_glove_ignored['tweet'].str.split(\" \",expand=True)\n",
    "\n",
    "# for evaluation\n",
    "dev_glove_X= dev_glove_trim.iloc[:,1:]\n",
    "dev_glove_X= dev_glove_X['tweet'].str.split(\" \",expand=True)\n",
    "dev_glove_Y= dev_glove_trim.iloc[:,:1]\n",
    "test_glove_X = test_glove_X['tweet'].str.split(\" \",expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each training set, transform each string type column into numbers.\n",
    "# May cause memory overhead.\n",
    "for dimension in range(300):\n",
    "    train_glove_X[dimension]=pd.to_numeric(train_glove_X[dimension],errors='coerce')\n",
    "    train_glove_X_1t[dimension]=pd.to_numeric(train_glove_X_1t[dimension],errors='coerce')\n",
    "    train_glove_X_2t[dimension]=pd.to_numeric(train_glove_X_2t[dimension],errors='coerce')\n",
    "    train_glove_X_3t[dimension]=pd.to_numeric(train_glove_X_3t[dimension],errors='coerce')\n",
    "    train_glove_X_ignored[dimension]=pd.to_numeric(train_glove_X_ignored[dimension],errors='coerce')\n",
    "    dev_glove_X[dimension]= pd.to_numeric(dev_glove_X[dimension],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NaN in glove_X_1t? False\n",
      " NaN in glove_X_2t? False\n",
      " NaN in glove_X_3t? False\n",
      " NaN in dev_glove_X? False\n"
     ]
    }
   ],
   "source": [
    "#just in case, check if there is anything transformed into NaN\n",
    "print(f\" NaN in glove_X_1t? {train_glove_X_1t.isnull().values.any()}\")\n",
    "print(f\" NaN in glove_X_2t? {train_glove_X_2t.isnull().values.any()}\")\n",
    "print(f\" NaN in glove_X_3t? {train_glove_X_3t.isnull().values.any()}\")\n",
    "print(f\" NaN in dev_glove_X? {dev_glove_X.isnull().values.any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>44118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NORTHEAST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>17189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           region\n",
       "count       44118\n",
       "unique          4\n",
       "top     NORTHEAST\n",
       "freq        17189"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_glove_Y_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.019811</td>\n",
       "      <td>-0.116162</td>\n",
       "      <td>-0.055503</td>\n",
       "      <td>0.018222</td>\n",
       "      <td>-0.146421</td>\n",
       "      <td>0.081041</td>\n",
       "      <td>-0.145029</td>\n",
       "      <td>0.053756</td>\n",
       "      <td>-0.046450</td>\n",
       "      <td>-0.775886</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027007</td>\n",
       "      <td>-0.049356</td>\n",
       "      <td>-0.069704</td>\n",
       "      <td>-0.088946</td>\n",
       "      <td>0.081240</td>\n",
       "      <td>-0.241222</td>\n",
       "      <td>0.095841</td>\n",
       "      <td>-0.083739</td>\n",
       "      <td>0.018884</td>\n",
       "      <td>0.280380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.173262</td>\n",
       "      <td>0.175441</td>\n",
       "      <td>0.148550</td>\n",
       "      <td>0.166573</td>\n",
       "      <td>0.200535</td>\n",
       "      <td>0.170643</td>\n",
       "      <td>0.166701</td>\n",
       "      <td>0.174294</td>\n",
       "      <td>0.160285</td>\n",
       "      <td>0.499090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147861</td>\n",
       "      <td>0.148019</td>\n",
       "      <td>0.181975</td>\n",
       "      <td>0.184948</td>\n",
       "      <td>0.147736</td>\n",
       "      <td>0.290106</td>\n",
       "      <td>0.168879</td>\n",
       "      <td>0.192571</td>\n",
       "      <td>0.158279</td>\n",
       "      <td>0.191895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.906210</td>\n",
       "      <td>-1.174200</td>\n",
       "      <td>-1.078300</td>\n",
       "      <td>-1.237800</td>\n",
       "      <td>-1.387000</td>\n",
       "      <td>-1.114300</td>\n",
       "      <td>-1.272800</td>\n",
       "      <td>-1.087100</td>\n",
       "      <td>-1.224500</td>\n",
       "      <td>-2.382300</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.215100</td>\n",
       "      <td>-1.157000</td>\n",
       "      <td>-0.974750</td>\n",
       "      <td>-1.328900</td>\n",
       "      <td>-1.106400</td>\n",
       "      <td>-1.869800</td>\n",
       "      <td>-0.983290</td>\n",
       "      <td>-1.461800</td>\n",
       "      <td>-1.776200</td>\n",
       "      <td>-1.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.080751</td>\n",
       "      <td>-0.224316</td>\n",
       "      <td>-0.135373</td>\n",
       "      <td>-0.078404</td>\n",
       "      <td>-0.261205</td>\n",
       "      <td>-0.011305</td>\n",
       "      <td>-0.241523</td>\n",
       "      <td>-0.030829</td>\n",
       "      <td>-0.133783</td>\n",
       "      <td>-1.095100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104703</td>\n",
       "      <td>-0.138190</td>\n",
       "      <td>-0.176493</td>\n",
       "      <td>-0.194531</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>-0.405490</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>-0.197348</td>\n",
       "      <td>-0.068244</td>\n",
       "      <td>0.174268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.012638</td>\n",
       "      <td>-0.118082</td>\n",
       "      <td>-0.046733</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>-0.132667</td>\n",
       "      <td>0.083733</td>\n",
       "      <td>-0.146794</td>\n",
       "      <td>0.063217</td>\n",
       "      <td>-0.043736</td>\n",
       "      <td>-0.802765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024192</td>\n",
       "      <td>-0.050827</td>\n",
       "      <td>-0.075509</td>\n",
       "      <td>-0.083739</td>\n",
       "      <td>0.081367</td>\n",
       "      <td>-0.229081</td>\n",
       "      <td>0.091577</td>\n",
       "      <td>-0.089761</td>\n",
       "      <td>0.022194</td>\n",
       "      <td>0.284711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.108994</td>\n",
       "      <td>-0.011089</td>\n",
       "      <td>0.034235</td>\n",
       "      <td>0.100670</td>\n",
       "      <td>-0.011404</td>\n",
       "      <td>0.171330</td>\n",
       "      <td>-0.049448</td>\n",
       "      <td>0.150345</td>\n",
       "      <td>0.047269</td>\n",
       "      <td>-0.516758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053427</td>\n",
       "      <td>0.035827</td>\n",
       "      <td>0.031504</td>\n",
       "      <td>0.023586</td>\n",
       "      <td>0.159982</td>\n",
       "      <td>-0.058950</td>\n",
       "      <td>0.184838</td>\n",
       "      <td>0.024711</td>\n",
       "      <td>0.107255</td>\n",
       "      <td>0.393726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.222200</td>\n",
       "      <td>1.432100</td>\n",
       "      <td>1.325200</td>\n",
       "      <td>1.336400</td>\n",
       "      <td>0.865570</td>\n",
       "      <td>0.985550</td>\n",
       "      <td>0.878430</td>\n",
       "      <td>1.289100</td>\n",
       "      <td>1.043100</td>\n",
       "      <td>1.366000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788900</td>\n",
       "      <td>0.971050</td>\n",
       "      <td>1.752100</td>\n",
       "      <td>1.184800</td>\n",
       "      <td>0.968240</td>\n",
       "      <td>1.392600</td>\n",
       "      <td>1.203100</td>\n",
       "      <td>1.178200</td>\n",
       "      <td>1.103100</td>\n",
       "      <td>1.851100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2             3             4    \\\n",
       "count  44118.000000  44118.000000  44118.000000  44118.000000  44118.000000   \n",
       "mean       0.019811     -0.116162     -0.055503      0.018222     -0.146421   \n",
       "std        0.173262      0.175441      0.148550      0.166573      0.200535   \n",
       "min       -0.906210     -1.174200     -1.078300     -1.237800     -1.387000   \n",
       "25%       -0.080751     -0.224316     -0.135373     -0.078404     -0.261205   \n",
       "50%        0.012638     -0.118082     -0.046733      0.007882     -0.132667   \n",
       "75%        0.108994     -0.011089      0.034235      0.100670     -0.011404   \n",
       "max        1.222200      1.432100      1.325200      1.336400      0.865570   \n",
       "\n",
       "                5             6             7             8             9    \\\n",
       "count  44118.000000  44118.000000  44118.000000  44118.000000  44118.000000   \n",
       "mean       0.081041     -0.145029      0.053756     -0.046450     -0.775886   \n",
       "std        0.170643      0.166701      0.174294      0.160285      0.499090   \n",
       "min       -1.114300     -1.272800     -1.087100     -1.224500     -2.382300   \n",
       "25%       -0.011305     -0.241523     -0.030829     -0.133783     -1.095100   \n",
       "50%        0.083733     -0.146794      0.063217     -0.043736     -0.802765   \n",
       "75%        0.171330     -0.049448      0.150345      0.047269     -0.516758   \n",
       "max        0.985550      0.878430      1.289100      1.043100      1.366000   \n",
       "\n",
       "       ...           290           291           292           293  \\\n",
       "count  ...  44118.000000  44118.000000  44118.000000  44118.000000   \n",
       "mean   ...     -0.027007     -0.049356     -0.069704     -0.088946   \n",
       "std    ...      0.147861      0.148019      0.181975      0.184948   \n",
       "min    ...     -1.215100     -1.157000     -0.974750     -1.328900   \n",
       "25%    ...     -0.104703     -0.138190     -0.176493     -0.194531   \n",
       "50%    ...     -0.024192     -0.050827     -0.075509     -0.083739   \n",
       "75%    ...      0.053427      0.035827      0.031504      0.023586   \n",
       "max    ...      0.788900      0.971050      1.752100      1.184800   \n",
       "\n",
       "                294           295           296           297           298  \\\n",
       "count  44118.000000  44118.000000  44118.000000  44118.000000  44118.000000   \n",
       "mean       0.081240     -0.241222      0.095841     -0.083739      0.018884   \n",
       "std        0.147736      0.290106      0.168879      0.192571      0.158279   \n",
       "min       -1.106400     -1.869800     -0.983290     -1.461800     -1.776200   \n",
       "25%        0.006780     -0.405490      0.000380     -0.197348     -0.068244   \n",
       "50%        0.081367     -0.229081      0.091577     -0.089761      0.022194   \n",
       "75%        0.159982     -0.058950      0.184838      0.024711      0.107255   \n",
       "max        0.968240      1.392600      1.203100      1.178200      1.103100   \n",
       "\n",
       "                299  \n",
       "count  44118.000000  \n",
       "mean       0.280380  \n",
       "std        0.191895  \n",
       "min       -1.231200  \n",
       "25%        0.174268  \n",
       "50%        0.284711  \n",
       "75%        0.393726  \n",
       "max        1.851100  \n",
       "\n",
       "[8 rows x 300 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_glove_X_1t.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "      <td>44118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.020789</td>\n",
       "      <td>-0.117484</td>\n",
       "      <td>-0.054701</td>\n",
       "      <td>0.017462</td>\n",
       "      <td>-0.149266</td>\n",
       "      <td>0.081632</td>\n",
       "      <td>-0.145068</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>-0.047654</td>\n",
       "      <td>-0.776745</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026576</td>\n",
       "      <td>-0.048337</td>\n",
       "      <td>-0.069378</td>\n",
       "      <td>-0.091201</td>\n",
       "      <td>0.081932</td>\n",
       "      <td>-0.241740</td>\n",
       "      <td>0.096604</td>\n",
       "      <td>-0.085328</td>\n",
       "      <td>0.019947</td>\n",
       "      <td>0.282378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.175113</td>\n",
       "      <td>0.173588</td>\n",
       "      <td>0.147723</td>\n",
       "      <td>0.164530</td>\n",
       "      <td>0.201050</td>\n",
       "      <td>0.170548</td>\n",
       "      <td>0.164581</td>\n",
       "      <td>0.173396</td>\n",
       "      <td>0.160497</td>\n",
       "      <td>0.501160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147737</td>\n",
       "      <td>0.147993</td>\n",
       "      <td>0.182997</td>\n",
       "      <td>0.186131</td>\n",
       "      <td>0.147919</td>\n",
       "      <td>0.291193</td>\n",
       "      <td>0.171513</td>\n",
       "      <td>0.192672</td>\n",
       "      <td>0.156150</td>\n",
       "      <td>0.190761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.986180</td>\n",
       "      <td>-1.025100</td>\n",
       "      <td>-1.078300</td>\n",
       "      <td>-1.022000</td>\n",
       "      <td>-1.387000</td>\n",
       "      <td>-1.114300</td>\n",
       "      <td>-1.280400</td>\n",
       "      <td>-1.087100</td>\n",
       "      <td>-1.224500</td>\n",
       "      <td>-2.484300</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.235600</td>\n",
       "      <td>-0.911070</td>\n",
       "      <td>-1.156600</td>\n",
       "      <td>-1.328900</td>\n",
       "      <td>-1.051400</td>\n",
       "      <td>-1.869800</td>\n",
       "      <td>-1.195000</td>\n",
       "      <td>-1.461800</td>\n",
       "      <td>-1.563000</td>\n",
       "      <td>-1.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.080225</td>\n",
       "      <td>-0.224089</td>\n",
       "      <td>-0.135527</td>\n",
       "      <td>-0.078343</td>\n",
       "      <td>-0.266611</td>\n",
       "      <td>-0.009671</td>\n",
       "      <td>-0.242103</td>\n",
       "      <td>-0.032271</td>\n",
       "      <td>-0.135024</td>\n",
       "      <td>-1.097021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102667</td>\n",
       "      <td>-0.137949</td>\n",
       "      <td>-0.175461</td>\n",
       "      <td>-0.196198</td>\n",
       "      <td>0.006608</td>\n",
       "      <td>-0.406956</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.196358</td>\n",
       "      <td>-0.066662</td>\n",
       "      <td>0.173861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.013237</td>\n",
       "      <td>-0.119534</td>\n",
       "      <td>-0.046735</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>-0.135933</td>\n",
       "      <td>0.083633</td>\n",
       "      <td>-0.148235</td>\n",
       "      <td>0.061775</td>\n",
       "      <td>-0.045323</td>\n",
       "      <td>-0.804674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023372</td>\n",
       "      <td>-0.049372</td>\n",
       "      <td>-0.075001</td>\n",
       "      <td>-0.086146</td>\n",
       "      <td>0.082088</td>\n",
       "      <td>-0.228694</td>\n",
       "      <td>0.093322</td>\n",
       "      <td>-0.090727</td>\n",
       "      <td>0.022750</td>\n",
       "      <td>0.286677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.108559</td>\n",
       "      <td>-0.011351</td>\n",
       "      <td>0.034787</td>\n",
       "      <td>0.099605</td>\n",
       "      <td>-0.014296</td>\n",
       "      <td>0.171166</td>\n",
       "      <td>-0.050384</td>\n",
       "      <td>0.149478</td>\n",
       "      <td>0.046580</td>\n",
       "      <td>-0.523205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054296</td>\n",
       "      <td>0.037724</td>\n",
       "      <td>0.032210</td>\n",
       "      <td>0.022826</td>\n",
       "      <td>0.160850</td>\n",
       "      <td>-0.058119</td>\n",
       "      <td>0.187455</td>\n",
       "      <td>0.021574</td>\n",
       "      <td>0.106553</td>\n",
       "      <td>0.392885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.222200</td>\n",
       "      <td>1.100900</td>\n",
       "      <td>1.006600</td>\n",
       "      <td>1.336400</td>\n",
       "      <td>0.831840</td>\n",
       "      <td>1.051300</td>\n",
       "      <td>0.919770</td>\n",
       "      <td>1.289100</td>\n",
       "      <td>1.043100</td>\n",
       "      <td>1.473800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858950</td>\n",
       "      <td>0.971050</td>\n",
       "      <td>1.752100</td>\n",
       "      <td>1.175900</td>\n",
       "      <td>0.968240</td>\n",
       "      <td>1.270500</td>\n",
       "      <td>1.294200</td>\n",
       "      <td>1.178200</td>\n",
       "      <td>1.103100</td>\n",
       "      <td>1.851100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2             3             4    \\\n",
       "count  44118.000000  44118.000000  44118.000000  44118.000000  44118.000000   \n",
       "mean       0.020789     -0.117484     -0.054701      0.017462     -0.149266   \n",
       "std        0.175113      0.173588      0.147723      0.164530      0.201050   \n",
       "min       -0.986180     -1.025100     -1.078300     -1.022000     -1.387000   \n",
       "25%       -0.080225     -0.224089     -0.135527     -0.078343     -0.266611   \n",
       "50%        0.013237     -0.119534     -0.046735      0.007359     -0.135933   \n",
       "75%        0.108559     -0.011351      0.034787      0.099605     -0.014296   \n",
       "max        1.222200      1.100900      1.006600      1.336400      0.831840   \n",
       "\n",
       "                5             6             7             8             9    \\\n",
       "count  44118.000000  44118.000000  44118.000000  44118.000000  44118.000000   \n",
       "mean       0.081632     -0.145068      0.051724     -0.047654     -0.776745   \n",
       "std        0.170548      0.164581      0.173396      0.160497      0.501160   \n",
       "min       -1.114300     -1.280400     -1.087100     -1.224500     -2.484300   \n",
       "25%       -0.009671     -0.242103     -0.032271     -0.135024     -1.097021   \n",
       "50%        0.083633     -0.148235      0.061775     -0.045323     -0.804674   \n",
       "75%        0.171166     -0.050384      0.149478      0.046580     -0.523205   \n",
       "max        1.051300      0.919770      1.289100      1.043100      1.473800   \n",
       "\n",
       "       ...           290           291           292           293  \\\n",
       "count  ...  44118.000000  44118.000000  44118.000000  44118.000000   \n",
       "mean   ...     -0.026576     -0.048337     -0.069378     -0.091201   \n",
       "std    ...      0.147737      0.147993      0.182997      0.186131   \n",
       "min    ...     -1.235600     -0.911070     -1.156600     -1.328900   \n",
       "25%    ...     -0.102667     -0.137949     -0.175461     -0.196198   \n",
       "50%    ...     -0.023372     -0.049372     -0.075001     -0.086146   \n",
       "75%    ...      0.054296      0.037724      0.032210      0.022826   \n",
       "max    ...      0.858950      0.971050      1.752100      1.175900   \n",
       "\n",
       "                294           295           296           297           298  \\\n",
       "count  44118.000000  44118.000000  44118.000000  44118.000000  44118.000000   \n",
       "mean       0.081932     -0.241740      0.096604     -0.085328      0.019947   \n",
       "std        0.147919      0.291193      0.171513      0.192672      0.156150   \n",
       "min       -1.051400     -1.869800     -1.195000     -1.461800     -1.563000   \n",
       "25%        0.006608     -0.406956     -0.000065     -0.196358     -0.066662   \n",
       "50%        0.082088     -0.228694      0.093322     -0.090727      0.022750   \n",
       "75%        0.160850     -0.058119      0.187455      0.021574      0.106553   \n",
       "max        0.968240      1.270500      1.294200      1.178200      1.103100   \n",
       "\n",
       "                299  \n",
       "count  44118.000000  \n",
       "mean       0.282378  \n",
       "std        0.190761  \n",
       "min       -1.231200  \n",
       "25%        0.173861  \n",
       "50%        0.286677  \n",
       "75%        0.392885  \n",
       "max        1.851100  \n",
       "\n",
       "[8 rows x 300 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_glove_X_2t.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "      <td>44117.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.018909</td>\n",
       "      <td>-0.116055</td>\n",
       "      <td>-0.056205</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>-0.147721</td>\n",
       "      <td>0.082228</td>\n",
       "      <td>-0.145211</td>\n",
       "      <td>0.052512</td>\n",
       "      <td>-0.046978</td>\n",
       "      <td>-0.778045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026644</td>\n",
       "      <td>-0.049045</td>\n",
       "      <td>-0.068981</td>\n",
       "      <td>-0.090067</td>\n",
       "      <td>0.081209</td>\n",
       "      <td>-0.241283</td>\n",
       "      <td>0.096116</td>\n",
       "      <td>-0.086170</td>\n",
       "      <td>0.018546</td>\n",
       "      <td>0.282863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.173759</td>\n",
       "      <td>0.174424</td>\n",
       "      <td>0.149203</td>\n",
       "      <td>0.164817</td>\n",
       "      <td>0.200223</td>\n",
       "      <td>0.172222</td>\n",
       "      <td>0.165643</td>\n",
       "      <td>0.173614</td>\n",
       "      <td>0.160512</td>\n",
       "      <td>0.498120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148421</td>\n",
       "      <td>0.149356</td>\n",
       "      <td>0.181765</td>\n",
       "      <td>0.184271</td>\n",
       "      <td>0.147123</td>\n",
       "      <td>0.291202</td>\n",
       "      <td>0.171352</td>\n",
       "      <td>0.191539</td>\n",
       "      <td>0.157090</td>\n",
       "      <td>0.190907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.906210</td>\n",
       "      <td>-1.025100</td>\n",
       "      <td>-1.078300</td>\n",
       "      <td>-1.237800</td>\n",
       "      <td>-1.387000</td>\n",
       "      <td>-1.114300</td>\n",
       "      <td>-1.272800</td>\n",
       "      <td>-1.087100</td>\n",
       "      <td>-1.224500</td>\n",
       "      <td>-2.680100</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.215100</td>\n",
       "      <td>-1.015465</td>\n",
       "      <td>-0.974750</td>\n",
       "      <td>-1.457700</td>\n",
       "      <td>-1.116400</td>\n",
       "      <td>-1.869800</td>\n",
       "      <td>-1.195000</td>\n",
       "      <td>-1.461800</td>\n",
       "      <td>-1.079400</td>\n",
       "      <td>-1.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.081107</td>\n",
       "      <td>-0.223563</td>\n",
       "      <td>-0.137014</td>\n",
       "      <td>-0.077182</td>\n",
       "      <td>-0.264147</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>-0.242690</td>\n",
       "      <td>-0.031712</td>\n",
       "      <td>-0.134343</td>\n",
       "      <td>-1.096093</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104978</td>\n",
       "      <td>-0.138784</td>\n",
       "      <td>-0.175347</td>\n",
       "      <td>-0.194944</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>-0.408007</td>\n",
       "      <td>-0.001595</td>\n",
       "      <td>-0.199763</td>\n",
       "      <td>-0.066849</td>\n",
       "      <td>0.175939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.011609</td>\n",
       "      <td>-0.119112</td>\n",
       "      <td>-0.048204</td>\n",
       "      <td>0.007793</td>\n",
       "      <td>-0.134050</td>\n",
       "      <td>0.085660</td>\n",
       "      <td>-0.147773</td>\n",
       "      <td>0.062474</td>\n",
       "      <td>-0.044423</td>\n",
       "      <td>-0.807782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023771</td>\n",
       "      <td>-0.051057</td>\n",
       "      <td>-0.075540</td>\n",
       "      <td>-0.085657</td>\n",
       "      <td>0.081367</td>\n",
       "      <td>-0.228991</td>\n",
       "      <td>0.093573</td>\n",
       "      <td>-0.091506</td>\n",
       "      <td>0.022566</td>\n",
       "      <td>0.286881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.107583</td>\n",
       "      <td>-0.010750</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.099480</td>\n",
       "      <td>-0.013127</td>\n",
       "      <td>0.172725</td>\n",
       "      <td>-0.050639</td>\n",
       "      <td>0.149675</td>\n",
       "      <td>0.046925</td>\n",
       "      <td>-0.523532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054951</td>\n",
       "      <td>0.038346</td>\n",
       "      <td>0.033044</td>\n",
       "      <td>0.023087</td>\n",
       "      <td>0.159853</td>\n",
       "      <td>-0.056642</td>\n",
       "      <td>0.185861</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.106446</td>\n",
       "      <td>0.396782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.308900</td>\n",
       "      <td>1.085100</td>\n",
       "      <td>1.020000</td>\n",
       "      <td>1.336400</td>\n",
       "      <td>0.874250</td>\n",
       "      <td>1.051300</td>\n",
       "      <td>0.878430</td>\n",
       "      <td>1.289100</td>\n",
       "      <td>1.043100</td>\n",
       "      <td>1.366000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788420</td>\n",
       "      <td>1.114800</td>\n",
       "      <td>1.752100</td>\n",
       "      <td>1.383700</td>\n",
       "      <td>0.968240</td>\n",
       "      <td>1.091200</td>\n",
       "      <td>1.294200</td>\n",
       "      <td>1.153500</td>\n",
       "      <td>1.561600</td>\n",
       "      <td>1.851100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2             3             4    \\\n",
       "count  44117.000000  44117.000000  44117.000000  44117.000000  44117.000000   \n",
       "mean       0.018909     -0.116055     -0.056205      0.017986     -0.147721   \n",
       "std        0.173759      0.174424      0.149203      0.164817      0.200223   \n",
       "min       -0.906210     -1.025100     -1.078300     -1.237800     -1.387000   \n",
       "25%       -0.081107     -0.223563     -0.137014     -0.077182     -0.264147   \n",
       "50%        0.011609     -0.119112     -0.048204      0.007793     -0.134050   \n",
       "75%        0.107583     -0.010750      0.033195      0.099480     -0.013127   \n",
       "max        1.308900      1.085100      1.020000      1.336400      0.874250   \n",
       "\n",
       "                5             6             7             8             9    \\\n",
       "count  44117.000000  44117.000000  44117.000000  44117.000000  44117.000000   \n",
       "mean       0.082228     -0.145211      0.052512     -0.046978     -0.778045   \n",
       "std        0.172222      0.165643      0.173614      0.160512      0.498120   \n",
       "min       -1.114300     -1.272800     -1.087100     -1.224500     -2.680100   \n",
       "25%       -0.009114     -0.242690     -0.031712     -0.134343     -1.096093   \n",
       "50%        0.085660     -0.147773      0.062474     -0.044423     -0.807782   \n",
       "75%        0.172725     -0.050639      0.149675      0.046925     -0.523532   \n",
       "max        1.051300      0.878430      1.289100      1.043100      1.366000   \n",
       "\n",
       "       ...           290           291           292           293  \\\n",
       "count  ...  44117.000000  44117.000000  44117.000000  44117.000000   \n",
       "mean   ...     -0.026644     -0.049045     -0.068981     -0.090067   \n",
       "std    ...      0.148421      0.149356      0.181765      0.184271   \n",
       "min    ...     -1.215100     -1.015465     -0.974750     -1.457700   \n",
       "25%    ...     -0.104978     -0.138784     -0.175347     -0.194944   \n",
       "50%    ...     -0.023771     -0.051057     -0.075540     -0.085657   \n",
       "75%    ...      0.054951      0.038346      0.033044      0.023087   \n",
       "max    ...      0.788420      1.114800      1.752100      1.383700   \n",
       "\n",
       "                294           295           296           297           298  \\\n",
       "count  44117.000000  44117.000000  44117.000000  44117.000000  44117.000000   \n",
       "mean       0.081209     -0.241283      0.096116     -0.086170      0.018546   \n",
       "std        0.147123      0.291202      0.171352      0.191539      0.157090   \n",
       "min       -1.116400     -1.869800     -1.195000     -1.461800     -1.079400   \n",
       "25%        0.006000     -0.408007     -0.001595     -0.199763     -0.066849   \n",
       "50%        0.081367     -0.228991      0.093573     -0.091506      0.022566   \n",
       "75%        0.159853     -0.056642      0.185861      0.021682      0.106446   \n",
       "max        0.968240      1.091200      1.294200      1.153500      1.561600   \n",
       "\n",
       "                299  \n",
       "count  44117.000000  \n",
       "mean       0.282863  \n",
       "std        0.190907  \n",
       "min       -1.231200  \n",
       "25%        0.175939  \n",
       "50%        0.286881  \n",
       "75%        0.396782  \n",
       "max        1.851100  \n",
       "\n",
       "[8 rows x 300 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_glove_X_3t.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>1442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1       2       3       4       5       6       7       8    \\\n",
       "count  1442.0  1442.0  1442.0  1442.0  1442.0  1442.0  1442.0  1442.0  1442.0   \n",
       "mean      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "std       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "min       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "50%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "75%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "max       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "          9    ...     290     291     292     293     294     295     296  \\\n",
       "count  1442.0  ...  1442.0  1442.0  1442.0  1442.0  1442.0  1442.0  1442.0   \n",
       "mean      0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "std       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "min       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25%       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "50%       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "75%       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "max       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "          297     298     299  \n",
       "count  1442.0  1442.0  1442.0  \n",
       "mean      0.0     0.0     0.0  \n",
       "std       0.0     0.0     0.0  \n",
       "min       0.0     0.0     0.0  \n",
       "25%       0.0     0.0     0.0  \n",
       "50%       0.0     0.0     0.0  \n",
       "75%       0.0     0.0     0.0  \n",
       "max       0.0     0.0     0.0  \n",
       "\n",
       "[8 rows x 300 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_glove_X_ignored.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------nb-------------\n",
      "cv for set 1: 0.2542275073524659, with acc: 0.2543859649122807\n",
      "cv for adding set 2: 0.24973928943442864, with acc: 0.24935400516795866\n",
      "cv for adding set 3: 0.263549217002582, with acc: 0.25740644196114876\n",
      "------------lr-------------\n",
      "cv for lr model, respect to set 1: 0.4422911283376399, with acc: 0.44770841833265335\n",
      "cv for lr model, respect to set 2: 0.4436737839430618, with acc: 0.4460764313885489\n",
      "cv for lr model, respect to set 3: 0.4443864056023968, with acc: 0.45653602919509484\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# since NB does not have many hyperparameters \n",
    "\n",
    "# Each model is now partially fit into the model, cv score will be evaluated each time when a new model is in.\n",
    "print(\"------------nb-------------\")\n",
    "clf = GaussianNB()\n",
    "clf.partial_fit(train_glove_X_1t,train_glove_Y_1['region'],classes = labels)\n",
    "print(f\"cv for set 1: {np.mean(cross_val_score(clf,train_glove_X_1t,train_glove_Y_1['region'],cv=5))}, with acc: {clf.score(train_glove_X_1t,train_glove_Y_1['region'])}\")\n",
    "clf.partial_fit(train_glove_X_2t,train_glove_Y_2['region'])\n",
    "print(f\"cv for adding set 2: {np.mean(cross_val_score(clf,train_glove_X_2t,train_glove_Y_2['region'],cv=5))}, with acc: {clf.score(train_glove_X_2t,train_glove_Y_2['region'])}\")\n",
    "clf.partial_fit(train_glove_X_3t,train_glove_Y_3['region'])\n",
    "print(f\"cv for adding set 3: {np.mean(cross_val_score(clf,train_glove_X_3t,train_glove_Y_3['region'],cv=5))}, with acc: {clf.score(train_glove_X_3t,train_glove_Y_3['region'])}\")\n",
    "# The following is not used, because divide 0 problem, and also it does not make sense to put all 0 vectors in the model.\n",
    "#print(\"additional step, add 'invalid' instance into the model and see the performance change\")\n",
    "#clf.partial_fit(train_glove_X_ignored,train_glove_Y_ignored['region'])\n",
    "#print(f\"cv for adding model 4: {np.mean(cross_val_score(clf,train_glove_X_ignored,train_glove_Y_ignored['region'],cv=5))}, with acc: {clf.score(train_glove_X_ignored,train_glove_Y_ignored['region'])}\")\n",
    "print(\"------------lr-------------\")\n",
    "clf_lr = LogisticRegression(warm_start=True,max_iter=5000)\n",
    "clf_lr.fit(train_glove_X_1t,train_glove_Y_1['region'])\n",
    "clf_lr.fit(train_glove_X_2t,train_glove_Y_2['region'])\n",
    "clf_lr.fit(train_glove_X_3t,train_glove_Y_3['region'])\n",
    "print(f\"cv for lr model, respect to set 1: {np.mean(cross_val_score(clf_lr,train_glove_X_1t,train_glove_Y_1['region'],cv=3))}, with acc: {clf_lr.score(train_glove_X_1t,train_glove_Y_1['region'])}\")\n",
    "print(f\"cv for lr model, respect to set 2: {np.mean(cross_val_score(clf_lr,train_glove_X_2t,train_glove_Y_2['region'],cv=3))}, with acc: {clf_lr.score(train_glove_X_2t,train_glove_Y_2['region'])}\")\n",
    "print(f\"cv for lr model, respect to set 3: {np.mean(cross_val_score(clf_lr,train_glove_X_3t,train_glove_Y_3['region'],cv=3))}, with acc: {clf_lr.score(train_glove_X_3t,train_glove_Y_3['region'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------nb---------\n",
      "f1 score: 0.20916081251348945\n",
      "acc: 0.261010989010989\n",
      "--------lr---------\n",
      "f1 score: 0.2539933044603249\n",
      "acc: 0.4378901098901099\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"--------nb---------\")\n",
    "print(f\"f1 score: {f1_score(dev_glove_Y,clf.predict(dev_glove_X),average='macro',labels=labels)}\")\n",
    "print(f\"acc: {accuracy_score(dev_glove_Y,clf.predict(dev_glove_X))}\")\n",
    "print(\"--------lr---------\")\n",
    "print(f\"f1 score: {f1_score(dev_glove_Y,clf_lr.predict(dev_glove_X),average='macro',labels=labels)}\")\n",
    "print(f\"acc: {accuracy_score(dev_glove_Y,clf_lr.predict(dev_glove_X))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=clf_lr.predict(test_glove_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_prediction(prediction):\n",
    "    prediction = pd.DataFrame(prediction, columns=['region'])\n",
    "    id_list = list()\n",
    "    for i in range(len(prediction)):\n",
    "        id_list.append(i+1)\n",
    "    prediction['id']=id_list\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For output only:\n",
    "# columns_titles = [\"id\",\"region\"]\n",
    "# prediction=clf_lr.predict(test_glove_X)\n",
    "# prediction=reformat_prediction(prediction)\n",
    "# tweet_test_Y=prediction.reindex(columns=columns_titles)\n",
    "# tweet_test_Y.id.astype(int)\n",
    "# tweet_test_Y.to_csv('lr.csv',index=False,float_format='%.f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Experiment 3: KNN\n",
    "\n",
    "\n",
    "#### How: \n",
    "Use Glove300 for input of KNN. Since each tweet vector is really a set of 'meaning' of tweet with 300 characteristics that cannot be interpreted. It maybe possible that each geolocation labels are having somewhat range that can group a bounch of instances.\n",
    "\n",
    "## Since the data has been processed. Let's directly apply them to the knn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "n = train_glove_X[0].count()\n",
    "k = round(math.sqrt(n)/2)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=k)\n",
    "clf_knn_tfidf = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "clf_knn.fit(train_glove_X,train_glove_Y['region'])\n",
    "clf_knn_tfidf.fit(train_tfidf_X.todense(),train_glove_Y['region'])\n",
    "\n",
    "# very slow, not used here.\n",
    "#print(f\"cv for lr model, respect to set 1: {np.mean(cross_val_score(clf_knn,train_glove_X_1t,train_glove_Y_1['region'],cv=3))}, with acc: {clf_knn.score(train_glove_X_1t,train_glove_Y_1['region'])}\")\n",
    "#print(f\"cv for lr model, respect to set 2: {np.mean(cross_val_score(clf_knn,train_glove_X_2t,train_glove_Y_2['region'],cv=3))}, with acc: {clf_knn.score(train_glove_X_2t,train_glove_Y_2['region'])}\")\n",
    "#print(f\"cv for lr model, respect to set 3: {np.mean(cross_val_score(clf_knn,train_glove_X_3t,train_glove_Y_3['region'],cv=3))}, with acc: {clf_knn.score(train_glove_X_3t,train_glove_Y_3['region'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~20mins, very slow\n",
    "knn_acc=clf_knn.score(dev_glove_X,dev_glove_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn_tfidf.score(dev_tfidf_X_sparse.todense(),dev_tfidf_Y['region'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Experiment 4: Linear model (Naive Bayes+Lr) with Tf-idf\n",
    "\n",
    "\n",
    "#### How: \n",
    "Use Tf-idf for input of Naive Bayes. As a comparison with experiment 2\n",
    "\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_X['tweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "size_of_tweet = 120 # 120 words, thus 120 tfidf values\n",
    "tfidf_X_list = list()\n",
    "train_tfidf_X_index = list()\n",
    "dev_tfidf_X_index = list()\n",
    "test_tfidf_X_index = list()\n",
    "for tweet in train_tfidf_X['tweet']:\n",
    "    t = ast.literal_eval(tweet)\n",
    "    i = 0\n",
    "    tfidf_X_row = [0]*size_of_tweet\n",
    "    tfidf_X_row_index = list()\n",
    "    for word in t:\n",
    "        tfidf_X_row[i] = word[1]\n",
    "        tfidf_X_row_index.append(word[0])\n",
    "        i+=1\n",
    "    tfidf_X_list.append(tfidf_X_row)\n",
    "    train_tfidf_X_index.append(tfidf_X_row_index)\n",
    "\n",
    "train_tfidf_X_sparse = np.array(tfidf_X_list)\n",
    "train_tfidf_X_sparse = csr_matrix(train_tfidf_X_sparse)\n",
    "\n",
    "tfidf_X_list = list()\n",
    "for tweet in dev_tfidf_X['tweet']:\n",
    "    t = ast.literal_eval(tweet)\n",
    "    i = 0\n",
    "    tfidf_X_row = [0]*size_of_tweet\n",
    "    tfidf_X_row_index = list()\n",
    "    for word in t:\n",
    "        tfidf_X_row[i] = word[1]\n",
    "        tfidf_X_row_index.append(word[0])\n",
    "        i+=1\n",
    "    tfidf_X_list.append(tfidf_X_row)\n",
    "    dev_tfidf_X_index.append(tfidf_X_row_index)\n",
    "    \n",
    "dev_tfidf_X_sparse = np.array(tfidf_X_list)\n",
    "dev_tfidf_X_sparse = csr_matrix(dev_tfidf_X_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_X_list = list()\n",
    "for tweet in test_tfidf_X['tweet']:\n",
    "    t = ast.literal_eval(tweet)\n",
    "    i = 0\n",
    "    tfidf_X_row = [0]*size_of_tweet\n",
    "    tfidf_X_row_index = list()\n",
    "    for word in t:\n",
    "        tfidf_X_row[i] = word[1]\n",
    "        tfidf_X_row_index.append(word[0])\n",
    "        i+=1\n",
    "    tfidf_X_list.append(tfidf_X_row)\n",
    "    test_tfidf_X_index.append(tfidf_X_row_index)\n",
    "    \n",
    "test_tfidf_X_sparse = np.array(tfidf_X_list)\n",
    "test_tfidf_X_sparse = csr_matrix(test_tfidf_X_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for analytical\n",
    "df = pd.DataFrame(train_tfidf_X_sparse.todense())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(train_tfidf_X_sparse[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------nb-------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_tfidf_X_sparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8df872cf5053>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"------------nb-------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tfidf_X_sparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_tfidf_Y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'region'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"cv for tfidf set: {np.mean(cross_val_score(clf,train_tfidf_X_sparse.todense(),train_tfidf_Y['region'],cv=5))}, with acc: {clf.score(train_tfidf_X_sparse.todense(),train_tfidf_Y['region'])}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"------------lr-------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_tfidf_X_sparse' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# since NB does not have many hyperparameters \n",
    "\n",
    "# Each model is now partially fit into the model, cv score will be evaluated each time when a new model is in.\n",
    "print(\"------------nb-------------\")\n",
    "clf = BernoulliNB()\n",
    "clf.fit(train_tfidf_X_sparse.todense(),train_tfidf_Y['region'])\n",
    "print(f\"cv for tfidf set: {np.mean(cross_val_score(clf,train_tfidf_X_sparse.todense(),train_tfidf_Y['region'],cv=5))}, with acc: {clf.score(train_tfidf_X_sparse.todense(),train_tfidf_Y['region'])}\")\n",
    "print(\"------------lr-------------\")\n",
    "clf_lr = LogisticRegression(warm_start=True,max_iter=5000)\n",
    "clf_lr.fit(train_tfidf_X_sparse.todense(),train_tfidf_Y['region'])\n",
    "print(f\"cv for tfidf set: {np.mean(cross_val_score(clf_lr,train_tfidf_X_sparse.todense(),train_tfidf_Y['region'],cv=5))}, with acc: {clf_lr.score(train_tfidf_X_sparse.todense(),train_tfidf_Y['region'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr.score(dev_tfidf_X_sparse.todense(),dev_tfidf_Y['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(dev_tfidf_X_sparse.todense(),dev_tfidf_Y['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#X_full = np.concatenate((train_full_X['tweet'].values),axis=0)\n",
    "#svc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])\n",
    "#scores = cross_val_score(svc_tfidf,train_full_X['tweet'].values, train_tfidf_Y['region'], cv=5).mean()\n",
    "#print(scores)\n",
    "#print(svc_tfidf.score(dev_tfidf_X_sparse.todense(),dev_tfidf_Y['region']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Experiment 5: One model proposed by paper. (incomplete)\n",
    "\n",
    "\n",
    "#### How: \n",
    "Duplicate a paper's model.\n",
    "\n",
    "## Determining spatial focus and dispersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove really noisy words ('!','..','.','...')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "for w_id,w in vocab.items():\n",
    "    # Below regex pattern: 1. all english wordwith number at front or back 2. any non-english word but ascii char with number and english letters \n",
    "    #3. number+english letters with ascii char at the back\n",
    "    result = regex.match(\"[a-zA-Z]+[0-9]*|\\P{L}+[0-9]*[a-zA-Z]+|[0-9]*[a-zA-Z]+\\P{L}+\",w)\n",
    "    if not result:\n",
    "        for label in labeled_word_count.keys():\n",
    "            labeled_word_count[label].pop(w_id)\n",
    "            #print(f\"eject:{w_id}, word:{w}\")\n",
    "    #else:\n",
    "        #print(result.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opposite direction will get diff of 2. 0.1 for same location. 1 for different location but not opposite\n",
    "def get_distance(city,center,labels):\n",
    "    if (city == center):\n",
    "        return 0.1\n",
    "    else:\n",
    "        diff = labels.index(city) - labels.index(center)\n",
    "        if abs(diff) >=3:\n",
    "            return 2\n",
    "        else:\n",
    "            return abs(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fca(word, count,S,C,a,target_label,labels):\n",
    "    # Compute Sum of log(C*di^(-a)) for n times.\n",
    "    likelihood = count*math.log(C*np.power(get_distance(target_label,target_label,labels),-a))\n",
    "    p=0\n",
    "    # Compute Sum of log(C*di^(-a)) for n times.\n",
    "    for label in labels:\n",
    "        if label != target_label:\n",
    "            ct=S[label][str(word)]\n",
    "            p += ct*math.log(1-C*np.power(get_distance(target_label,label,labels),-a))\n",
    "    likelihood+=p\n",
    "    #print(f\"for word:{word}, likelihood:{likelihood}\")\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_C = np.arange(0.1,1,0.01).tolist()\n",
    "possible_a = np.arange(0.1,2,0.1).tolist() # 2 here means the largest distance in get_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute likelihood for given center(target_label), returns C and a.\n",
    "# C is somewhere in between 0~1 (not equal to the bound)\n",
    "# a could be anything (hard to determine)\n",
    "def compute_likelihood(S,out,target_label,labels):\n",
    "    for word,count in S[target_label].items():\n",
    "        # brute force approach, not good to have but a quick solution for now.\n",
    "        likelihood=dict()\n",
    "        for c in possible_C:\n",
    "            for a in possible_a:\n",
    "                likelihood[fca(word,count,S,c,a,target_label,labels)] = [c,a]\n",
    "        sorted_likelihood = dict(sorted(likelihood.items(),reverse=True))\n",
    "        it = iter(sorted_likelihood.keys())\n",
    "        max_pair=next(it)\n",
    "        out[word]=[sorted_likelihood[max_pair],max_pair]\n",
    "        #print(f\"In label:{target_label}, for word:{vocab[word]}, found likelihood with C and a{sorted_likelihood[max_pair]},likelihood:{max_pair}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_Ca = labeled_word_count.fromkeys(labeled_word_count, {})\n",
    "for label in labels:\n",
    "    compute_likelihood(labeled_word_count,word_Ca[label],label,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_Ca_S=dict(sorted(word_Ca['SOUTH'].items(), key=lambda item: item[1][0],reverse=True))\n",
    "word_Ca_NE=dict(sorted(word_Ca['NORTHEAST'].items(), key=lambda item: item[1][0],reverse=True))\n",
    "word_Ca_MW=dict(sorted(word_Ca['MIDWEST'].items(), key=lambda item: item[1][0],reverse=True))\n",
    "word_Ca_W=dict(sorted(word_Ca['WEST'].items(), key=lambda item: item[1][0],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NORTHEAST', 'MIDWEST', 'WEST', 'SOUTH']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 words in NORTHEAST: lls, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 1830.5664602232357]\n",
      "top 10 words in NORTHEAST: jeezy, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 190.3033278753583]\n",
      "top 10 words in NORTHEAST: #famusextape, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 194.92698690612016]\n",
      "top 10 words in NORTHEAST: mph, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 366.34010869477146]\n",
      "top 10 words in NORTHEAST: ke5fvg, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 357.9186299484852]\n",
      "top 10 words in NORTHEAST: pct, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 357.9186299484852]\n",
      "top 10 words in NORTHEAST: dc, data [[C,a],likelihood]:[[0.9699999999999995, 1.9000000000000001], 335.557253100283]\n",
      "top 10 words in NORTHEAST: duke, data [[C,a],likelihood]:[[0.9699999999999995, 1.9000000000000001], 204.0848076422603]\n",
      "top 10 words in NORTHEAST: aunt, data [[C,a],likelihood]:[[0.9599999999999995, 1.9000000000000001], 137.17447072172774]\n",
      "top 10 words in NORTHEAST: #imjustsaying, data [[C,a],likelihood]:[[0.9599999999999995, 1.9000000000000001], 136.70609946800377]\n",
      "top 10 words in NORTHEAST: shawty, data [[C,a],likelihood]:[[0.9399999999999996, 1.9000000000000001], 254.20649979121703]\n",
      "top 10 words in MIDWEST: lls, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 1830.5664602232357]\n",
      "top 10 words in MIDWEST: jeezy, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 190.3033278753583]\n",
      "top 10 words in MIDWEST: #famusextape, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 194.92698690612016]\n",
      "top 10 words in MIDWEST: mph, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 366.34010869477146]\n",
      "top 10 words in MIDWEST: ke5fvg, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 357.9186299484852]\n",
      "top 10 words in MIDWEST: pct, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 357.9186299484852]\n",
      "top 10 words in MIDWEST: dc, data [[C,a],likelihood]:[[0.9699999999999995, 1.9000000000000001], 335.557253100283]\n",
      "top 10 words in MIDWEST: duke, data [[C,a],likelihood]:[[0.9699999999999995, 1.9000000000000001], 204.0848076422603]\n",
      "top 10 words in MIDWEST: aunt, data [[C,a],likelihood]:[[0.9599999999999995, 1.9000000000000001], 137.17447072172774]\n",
      "top 10 words in MIDWEST: #imjustsaying, data [[C,a],likelihood]:[[0.9599999999999995, 1.9000000000000001], 136.70609946800377]\n",
      "top 10 words in MIDWEST: shawty, data [[C,a],likelihood]:[[0.9399999999999996, 1.9000000000000001], 254.20649979121703]\n",
      "top 10 words in WEST: lls, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 1830.5664602232357]\n",
      "top 10 words in WEST: jeezy, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 190.3033278753583]\n",
      "top 10 words in WEST: #famusextape, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 194.92698690612016]\n",
      "top 10 words in WEST: mph, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 366.34010869477146]\n",
      "top 10 words in WEST: ke5fvg, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 357.9186299484852]\n",
      "top 10 words in WEST: pct, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 357.9186299484852]\n",
      "top 10 words in WEST: dc, data [[C,a],likelihood]:[[0.9699999999999995, 1.9000000000000001], 335.557253100283]\n",
      "top 10 words in WEST: duke, data [[C,a],likelihood]:[[0.9699999999999995, 1.9000000000000001], 204.0848076422603]\n",
      "top 10 words in WEST: aunt, data [[C,a],likelihood]:[[0.9599999999999995, 1.9000000000000001], 137.17447072172774]\n",
      "top 10 words in WEST: #imjustsaying, data [[C,a],likelihood]:[[0.9599999999999995, 1.9000000000000001], 136.70609946800377]\n",
      "top 10 words in WEST: shawty, data [[C,a],likelihood]:[[0.9399999999999996, 1.9000000000000001], 254.20649979121703]\n",
      "top 10 words in SOUTH: lls, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 1830.5664602232357]\n",
      "top 10 words in SOUTH: jeezy, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 190.3033278753583]\n",
      "top 10 words in SOUTH: #famusextape, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 194.92698690612016]\n",
      "top 10 words in SOUTH: mph, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 366.34010869477146]\n",
      "top 10 words in SOUTH: ke5fvg, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 357.9186299484852]\n",
      "top 10 words in SOUTH: pct, data [[C,a],likelihood]:[[0.9899999999999995, 1.9000000000000001], 357.9186299484852]\n",
      "top 10 words in SOUTH: dc, data [[C,a],likelihood]:[[0.9699999999999995, 1.9000000000000001], 335.557253100283]\n",
      "top 10 words in SOUTH: duke, data [[C,a],likelihood]:[[0.9699999999999995, 1.9000000000000001], 204.0848076422603]\n",
      "top 10 words in SOUTH: aunt, data [[C,a],likelihood]:[[0.9599999999999995, 1.9000000000000001], 137.17447072172774]\n",
      "top 10 words in SOUTH: #imjustsaying, data [[C,a],likelihood]:[[0.9599999999999995, 1.9000000000000001], 136.70609946800377]\n",
      "top 10 words in SOUTH: shawty, data [[C,a],likelihood]:[[0.9399999999999996, 1.9000000000000001], 254.20649979121703]\n"
     ]
    }
   ],
   "source": [
    "word_Cas=dict()\n",
    "word_Cas['NORTHEAST'] = word_Ca_NE\n",
    "word_Cas['SOUTH'] = word_Ca_S\n",
    "word_Cas['MIDWEST'] = word_Ca_MW\n",
    "word_Cas['WEST'] = word_Ca_W\n",
    "for label in labels:\n",
    "    i = 0\n",
    "    for word,values in word_Cas[label].items():\n",
    "        print(f\"top 10 words in {label}: {vocab[word]}, data [[C,a],likelihood]:{values}\")\n",
    "        if i == 10:\n",
    "            break\n",
    "        else:\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Experiment 6: CNN\n",
    "\n",
    "#### How: \n",
    "Duplicate a paper's model.\n",
    "pass inputs(as below) into a CNN model that comes with:\n",
    "- Convolutional layer with length of h*d, h= number of words in a tweet, d = dimension in GloVe. activation function = ReLU\n",
    "- Polling: 1-max, o_hat = max{[o1,o2,o3,...,on-h+1]}, in this case, with averaged GloVe300d, it will pick up max val.\n",
    "- Combination of other data: we only have username in this case, it may be used, see if i have got time or not.\n",
    "\n",
    "#### Input: \n",
    "- Glove 300d provided by teaching staff.\n",
    "- my own GloVe 200d. \n",
    "\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_MIDWEST', 'x0_NORTHEAST', 'x0_SOUTH', 'x0_WEST'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder(sparse=False)\n",
    "enc_dev = OneHotEncoder(sparse=False)\n",
    "ty = enc.fit_transform(train_glove_Y)\n",
    "dy = enc_dev.fit_transform(dev_glove_Y)\n",
    "enc_dev.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = train_glove_X.to_numpy()\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(tx, ty, test_size = 0.2, random_state=1)\n",
    "dx = dev_glove_X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = tx[..., None]\n",
    "dx = dx[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133795, 2, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 299, 128)     384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 298, 128)     512         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 297, 128)     640         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 2, 128)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 2, 128)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 2, 128)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2, 384)       0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 384)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 384)          0           global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          49280       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            516         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 51,332\n",
      "Trainable params: 51,332\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedded_sequences = keras.Input(shape=(300,1))\n",
    "\n",
    "f1 = layers.Conv1D(128, 2, activation=\"relu\")(embedded_sequences)\n",
    "f1 = layers.MaxPooling1D(100)(f1)\n",
    "f2 = layers.Conv1D(128, 3, activation=\"relu\")(embedded_sequences)\n",
    "f2 = layers.MaxPooling1D(100)(f2)\n",
    "f3 = layers.Conv1D(128, 4, activation=\"relu\")(embedded_sequences)\n",
    "f3 = layers.MaxPooling1D(100)(f3)\n",
    "x = layers.concatenate([f1,f2,f3])\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(labels), activation=\"softmax\")(x)\n",
    "model = keras.Model(embedded_sequences, preds)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1035/1035 [==============================] - 84s 81ms/step - loss: 1.2521 - acc: 0.3995 - val_loss: 1.2531 - val_acc: 0.4123\n",
      "Epoch 2/10\n",
      "1035/1035 [==============================] - 83s 81ms/step - loss: 1.2345 - acc: 0.4189 - val_loss: 1.2561 - val_acc: 0.4155\n",
      "Epoch 3/10\n",
      "1035/1035 [==============================] - 83s 80ms/step - loss: 1.2345 - acc: 0.4195 - val_loss: 1.2512 - val_acc: 0.4208\n",
      "Epoch 4/10\n",
      "1035/1035 [==============================] - 83s 80ms/step - loss: 1.2288 - acc: 0.4251 - val_loss: 1.2509 - val_acc: 0.4204\n",
      "Epoch 5/10\n",
      "1035/1035 [==============================] - 83s 80ms/step - loss: 1.2321 - acc: 0.4218 - val_loss: 1.2508 - val_acc: 0.4215\n",
      "Epoch 6/10\n",
      "1035/1035 [==============================] - 83s 81ms/step - loss: 1.2351 - acc: 0.4155 - val_loss: 1.2509 - val_acc: 0.4205\n",
      "Epoch 7/10\n",
      "1035/1035 [==============================] - 84s 81ms/step - loss: 1.2300 - acc: 0.4249 - val_loss: 1.2517 - val_acc: 0.4218\n",
      "Epoch 8/10\n",
      "1035/1035 [==============================] - 85s 83ms/step - loss: 1.2310 - acc: 0.4254 - val_loss: 1.2504 - val_acc: 0.4237\n",
      "Epoch 9/10\n",
      "1035/1035 [==============================] - 87s 84ms/step - loss: 1.2286 - acc: 0.4272 - val_loss: 1.2502 - val_acc: 0.4201\n",
      "Epoch 10/10\n",
      "1035/1035 [==============================] - 88s 85ms/step - loss: 1.2286 - acc: 0.4253 - val_loss: 1.2527 - val_acc: 0.4169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24a7822c970>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "#cb = [ModelCheckpoint(\"weights.h5\", save_best_only=True, save_weights_only=False)], callbacks=cb\n",
    "model.fit(tx, ty, validation_data=(dx,dy), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 3s 76ms/step - loss: 1.2527 - acc: 0.4169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2526755332946777, 0.4168791174888611]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dx,dy,batch_size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process from full text\n",
    "Note that i use a part of code from:https://keras.io/examples/nlp/pretrained_word_embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_twitter=train_full_X['tweet']\n",
    "X_train_twitter.to_numpy()\n",
    "X_dev_twitter=dev_full_X['tweet']\n",
    "X_dev_twitter.to_numpy()\n",
    "X_test_twitter=test_full_X['tweet']\n",
    "X_test_twitter.to_numpy()\n",
    "down_sample_limit = 116191\n",
    "#NORTHEAST\n",
    "#SOUTH\n",
    "i=0\n",
    "c_n=0\n",
    "c_s=0\n",
    "x_train = list()\n",
    "y_train = list()\n",
    "for tweet in X_train_twitter:\n",
    "    if y_train_twitter[i] == 'NORTHEAST':\n",
    "        if c_n != down_sample_limit:\n",
    "            x_train.append(tweet)\n",
    "            y_train.append(y_train_twitter[i])\n",
    "            c_n+=1\n",
    "    elif y_train_twitter[i] == 'SOUTH':\n",
    "        if c_s != down_sample_limit:\n",
    "            x_train.append(tweet)\n",
    "            y_train.append(y_train_twitter[i])\n",
    "            c_s+=1\n",
    "    else:\n",
    "        x_train.append(tweet)\n",
    "        y_train.append(y_train_twitter[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Watching LOST', '@USER_89a3500b i did',\n",
       "       'Maneuver so that I can put my team on, hopefully sooner so that we can live our dreams on',\n",
       "       ..., \"@USER_8cac2975 Haven't seen that one in ages. But I remeber\",\n",
       "       \"RT @USER_6d0753d3: You should all go and follow @USER_b7a77112. They're IN SPACE. They're TWEETING pix. and they're frakking ASTRONAUTS.\",\n",
       "       '@USER_4cf8655a Congratulations. (-:'], dtype='<U342')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_down = np.array(x_train)\n",
    "y_train_down = np.array(y_train)\n",
    "x_train_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         NORTHEAST\n",
       "1         NORTHEAST\n",
       "2         NORTHEAST\n",
       "3         NORTHEAST\n",
       "4         NORTHEAST\n",
       "            ...    \n",
       "133790        SOUTH\n",
       "133791        SOUTH\n",
       "133792        SOUTH\n",
       "133793        SOUTH\n",
       "133794        SOUTH\n",
       "Name: region, Length: 133795, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_twitter=train_full_Y['region']\n",
    "y_train_twitter.to_numpy()\n",
    "y_dev_twitter=dev_full_Y['region']\n",
    "y_dev_twitter.to_numpy()\n",
    "y_test_twitter=test_full_Y['region']\n",
    "y_test_twitter.to_numpy()\n",
    "y_train_twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=10000000, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_full_X['tweet'].to_numpy()).batch(128) # use entire training data for vocab\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1922, 1012, 17, 3, 9972]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "Glove vectors comes from: https://github.com/stanfordnlp/GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1177846 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "path_to_glove_file = os.path.join(\"G:/UNIMELB/ML/A3/emb/glove.twitter.27B.200d.txt\")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file,'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        word = word.translate(str.maketrans('', '', string.punctuation)).lower() #same way as keras Vectrization.\n",
    "        coefs = np.fromstring(coefs, sep=\" \")\n",
    "        if embeddings_index.get(word) is None: \n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 41331 words (101691 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 200\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "        #print(\"misses:\",word)\n",
    "        \n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model build and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_MIDWEST', 'x0_NORTHEAST', 'x0_SOUTH', 'x0_WEST'], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder(sparse=False)\n",
    "enc_dev = OneHotEncoder(sparse=False)\n",
    "y_train = enc.fit_transform(y_train.to_numpy().reshape(-1, 1))\n",
    "y_val = enc_dev.fit_transform(y_val.to_numpy().reshape(-1, 1))\n",
    "y_train_full = enc.fit_transform(y_train_twitter.to_numpy().reshape(-1,1))\n",
    "y_dev = enc_dev.fit_transform(y_dev_twitter.to_numpy().reshape(-1, 1))\n",
    "enc_dev.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = vectorizer(np.array([[s.translate(str.maketrans('', '', string.punctuation)).lower()] for s in x_train_down])).numpy()\n",
    "x_val = vectorizer(np.array([[s.translate(str.maketrans('', '', string.punctuation)).lower()] for s in X_val])).numpy()\n",
    "x_train_full = vectorizer(np.array([[s.translate(str.maketrans('', '', string.punctuation)).lower()] for s in X_train_twitter])).numpy()\n",
    "x_dev_full = vectorizer(np.array([[s.translate(str.maketrans('', '', string.punctuation)).lower()] for s in X_dev_twitter])).numpy()\n",
    "x_test_full = vectorizer(np.array([[s.translate(str.maketrans('', '', string.punctuation)).lower()] for s in X_test_twitter])).numpy()\n",
    "\n",
    "\n",
    "y_train = np.array(y_train_down)\n",
    "y_val = np.array(y_val)\n",
    "y_dev = np.array(y_dev)\n",
    "y_train_full = np.array(y_train_full)\n",
    "y_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 200)    28604800    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 128)    51328       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 128)    76928       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 128)    102528      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, None, 128)    0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, None, 128)    0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, None, 128)    0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 384)    0           max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          49280       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            516         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,885,380\n",
      "Trainable params: 28,885,380\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "f1 = layers.Conv1D(128, 2, activation=\"relu\")(embedded_sequences)\n",
    "f1 = layers.MaxPooling1D(100)(f1)\n",
    "f2 = layers.Conv1D(128, 3, activation=\"relu\")(embedded_sequences)\n",
    "f2 = layers.MaxPooling1D(100)(f2)\n",
    "f3 = layers.Conv1D(128, 4, activation=\"relu\")(embedded_sequences)\n",
    "f3 = layers.MaxPooling1D(100)(f3)\n",
    "x = layers.concatenate([f1,f2,f3])\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(labels), activation=\"softmax\")(x)\n",
    "model2 = keras.Model(int_sequences_input, preds)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1046/1046 [==============================] - 439s 420ms/step - loss: 1.2348 - acc: 0.4222 - val_loss: 1.2060 - val_acc: 0.4590\n",
      "Epoch 2/3\n",
      "1046/1046 [==============================] - 431s 412ms/step - loss: 0.8946 - acc: 0.6241 - val_loss: 1.2656 - val_acc: 0.4941\n",
      "Epoch 3/3\n",
      "1046/1046 [==============================] - 431s 412ms/step - loss: 0.4071 - acc: 0.8392 - val_loss: 1.3836 - val_acc: 0.4950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a86d548160>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"]\n",
    ")\n",
    "model2.fit(x_train_full, y_train_full, batch_size=128, epochs=3, validation_data=(x_dev_full, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 5s 61ms/step - loss: 1.3836 - acc: 0.4950: 0s - loss: 1.3672 - acc: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.383583426475525, 0.4949890971183777]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(x_dev_full,y_dev,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My own Glove 200d training (b1)\n",
    "Since above hit and miss is way too unbalanced (41064 words hit vs. 101956 misses), I will need to train my own GloVe with this!\n",
    "\n",
    "How: Use dev and train set, output each tweet as word corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start my own vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-164-ce0b25a4876a>:11: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  coefs = np.fromstring(coefs, sep=\" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1287266 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_my_glove_file = os.path.join(\"G:/UNIMELB/ML/A3/emb/my_glove_vectors_b1.txt\")\n",
    "#G:/UNIMELB/ML/A3/emb/my_glove_vectors_b2.txt\n",
    "#G:/UNIMELB/ML/A3/emb/my_glove_vectors_b5.txt\n",
    "\n",
    "# Use my own training set as complement set\n",
    "print(\"start my own vectors\")\n",
    "with open(path_to_my_glove_file,'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        word = word.translate(str.maketrans('', '', string.punctuation)).lower() #same way as keras Vectrization.\n",
    "        coefs = np.fromstring(coefs, sep=\" \")\n",
    "        if embeddings_index.get(word) is None: \n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 141352 words (1670 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 200\n",
    "hits = 0\n",
    "misses = 0\n",
    "x=0\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    elif embeddings_index.get(\"#\"+word) is not None:\n",
    "        embedding_matrix[i] = embeddings_index.get(\"#\"+word)\n",
    "        hits += 1\n",
    "    elif embeddings_index.get(\"@\"+word) is not None:\n",
    "        embedding_matrix[i] = embeddings_index.get(\"@\"+word)\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dev and train set as txt\n",
    "out = np.concatenate((dev_full_X['tweet'].values,train_full_X['tweet'].values, test_full_X['tweet'].values),axis=0)\n",
    "np.savetxt(r'my_glove_w_test.txt',out,fmt='%s',encoding='utf-8')\n",
    "# now, the out file will be moving to glove project copy in my local, and trained by my bash script in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model build and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 200)    28604800    input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, None, 128)    51328       embedding_6[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, None, 128)    76928       embedding_6[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, None, 128)    102528      embedding_6[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling1D) (None, None, 128)    0           conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling1D) (None, None, 128)    0           conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling1D) (None, None, 128)    0           conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, None, 384)    0           max_pooling1d_66[0][0]           \n",
      "                                                                 max_pooling1d_67[0][0]           \n",
      "                                                                 max_pooling1d_68[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 384)          0           concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 128)          49280       global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 128)          0           dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 4)            516         dropout_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 28,885,380\n",
      "Trainable params: 28,885,380\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "f1 = layers.Conv1D(128, 2, activation=\"relu\")(embedded_sequences)\n",
    "f1 = layers.MaxPooling1D(100)(f1)\n",
    "f2 = layers.Conv1D(128, 3, activation=\"relu\")(embedded_sequences)\n",
    "f2 = layers.MaxPooling1D(100)(f2)\n",
    "f3 = layers.Conv1D(128, 4, activation=\"relu\")(embedded_sequences)\n",
    "f3 = layers.MaxPooling1D(100)(f3)\n",
    "x = layers.concatenate([f1,f2,f3])\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(labels), activation=\"softmax\")(x)\n",
    "model3 = keras.Model(int_sequences_input, preds)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"]\n",
    ")\n",
    "model3.fit(x_train_full, y_train_full, batch_size=256, epochs=3, validation_data=(x_dev_full, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.evaluate(x_dev_full,y_dev,batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My own Glove 200d training (b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start my own vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-80d20b771fef>:11: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  coefs = np.fromstring(coefs, sep=\" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1201986 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_my_glove_file = os.path.join(\"G:/UNIMELB/ML/A3/emb/my_glove_vectors_b2.txt\")\n",
    "#G:/UNIMELB/ML/A3/emb/my_glove_vectors_b2.txt\n",
    "#G:/UNIMELB/ML/A3/emb/my_glove_vectors_b5.txt\n",
    "\n",
    "# Use my own training set as complement set\n",
    "print(\"start my own vectors\")\n",
    "with open(path_to_my_glove_file,'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        word = word.translate(str.maketrans('', '', string.punctuation)).lower() #same way as keras Vectrization.\n",
    "        coefs = np.fromstring(coefs, sep=\" \")\n",
    "        if embeddings_index.get(word) is None:\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 64234 words (78788 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 200\n",
    "hits = 0\n",
    "misses = 0\n",
    "x=0\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    elif embeddings_index.get(\"#\"+word) is not None:\n",
    "        embedding_matrix[i] = embeddings_index.get(\"#\"+word)\n",
    "        hits += 1\n",
    "    elif embeddings_index.get(\"@\"+word) is not None:\n",
    "        embedding_matrix[i] = embeddings_index.get(\"@\"+word)\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dev and train set as txt\n",
    "out = np.concatenate((dev_full_X['tweet'].values,train_full_X['tweet'].values),axis=0)\n",
    "#np.savetxt(r'my_glove.txt',out,fmt='%s',encoding='utf-8')\n",
    "# now, the out file will be moving to glove project copy in my local, and trained by my bash script in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model build and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 200)    28604800    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, None, 128)    51328       embedding[3][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 128)    76928       embedding[3][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, None, 128)    102528      embedding[3][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, None, 128)    0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, None, 128)    0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, None, 128)    0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, None, 384)    0           max_pooling1d_12[0][0]           \n",
      "                                                                 max_pooling1d_13[0][0]           \n",
      "                                                                 max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 384)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 128)          49280       global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,885,380\n",
      "Trainable params: 280,580\n",
      "Non-trainable params: 28,604,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "f1 = layers.Conv1D(128, 2, activation=\"relu\")(embedded_sequences)\n",
    "f1 = layers.MaxPooling1D(100)(f1)\n",
    "f2 = layers.Conv1D(128, 3, activation=\"relu\")(embedded_sequences)\n",
    "f2 = layers.MaxPooling1D(100)(f2)\n",
    "f3 = layers.Conv1D(128, 4, activation=\"relu\")(embedded_sequences)\n",
    "f3 = layers.MaxPooling1D(100)(f3)\n",
    "x = layers.concatenate([f1,f2,f3])\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(labels), activation=\"softmax\")(x)\n",
    "model3 = keras.Model(int_sequences_input, preds)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3345/3345 [==============================] - 482s 144ms/step - loss: 1.2475 - acc: 0.4139 - val_loss: 1.2021 - val_acc: 0.4493\n",
      "Epoch 2/3\n",
      "3345/3345 [==============================] - 476s 142ms/step - loss: 1.1941 - acc: 0.4566 - val_loss: 1.1908 - val_acc: 0.4607\n",
      "Epoch 3/3\n",
      "3345/3345 [==============================] - 473s 141ms/step - loss: 1.1673 - acc: 0.4758 - val_loss: 1.1846 - val_acc: 0.4670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x135a5756f40>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"]\n",
    ")\n",
    "model3.fit(x_train_full, y_train_full, batch_size=32, epochs=3, validation_data=(x_dev_full, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359/359 [==============================] - 14s 38ms/step - loss: 1.2184 - acc: 0.4578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2184489965438843, 0.4577777683734894]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(x_dev_full,y_dev,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My own Glove 200d training (b5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start my own vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-62-cebb12a85ae7>:11: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  coefs = np.fromstring(coefs, sep=\" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1201986 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_my_glove_file = os.path.join(\"G:/UNIMELB/ML/A3/emb/my_glove_vectors_b5.txt\")\n",
    "#G:/UNIMELB/ML/A3/emb/my_glove_vectors_b2.txt\n",
    "#G:/UNIMELB/ML/A3/emb/my_glove_vectors_b5.txt\n",
    "\n",
    "# Use my own training set as complement set\n",
    "print(\"start my own vectors\")\n",
    "with open(path_to_my_glove_file,'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        word = word.translate(str.maketrans('', '', string.punctuation)).lower() #same way as keras Vectrization.\n",
    "        coefs = np.fromstring(coefs, sep=\" \")\n",
    "        if embeddings_index.get(word) is None:\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 64234 words (78788 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 200\n",
    "hits = 0\n",
    "misses = 0\n",
    "x=0\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    elif embeddings_index.get(\"#\"+word) is not None:\n",
    "        embedding_matrix[i] = embeddings_index.get(\"#\"+word)\n",
    "        hits += 1\n",
    "    elif embeddings_index.get(\"@\"+word) is not None:\n",
    "        embedding_matrix[i] = embeddings_index.get(\"@\"+word)\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dev and train set as txt\n",
    "out = np.concatenate((dev_full_X['tweet'].values,train_full_X['tweet'].values),axis=0)\n",
    "#np.savetxt(r'my_glove.txt',out,fmt='%s',encoding='utf-8')\n",
    "# now, the out file will be moving to glove project copy in my local, and trained by my bash script in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model build and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 200)     28604800    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 199, 128)     51328       embedding_1[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 198, 128)     76928       embedding_1[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 197, 128)     102528      embedding_1[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 1, 128)       0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1, 128)       0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 1, 128)       0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1, 384)       0           max_pooling1d_12[0][0]           \n",
      "                                                                 max_pooling1d_13[0][0]           \n",
      "                                                                 max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 384)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 384)          0           global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 4)            1540        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,837,124\n",
      "Trainable params: 28,837,124\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(200,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "f1 = layers.Conv1D(128, 2, activation=\"relu\")(embedded_sequences)\n",
    "f1 = layers.MaxPooling1D(198)(f1)\n",
    "f2 = layers.Conv1D(128, 3, activation=\"relu\")(embedded_sequences)\n",
    "f2 = layers.MaxPooling1D(197)(f2)\n",
    "f3 = layers.Conv1D(128, 4, activation=\"relu\")(embedded_sequences)\n",
    "f3 = layers.MaxPooling1D(196)(f3)\n",
    "x = layers.concatenate([f1,f2,f3])\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "#x = layers.Dense(150, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(labels), activation=\"softmax\")(x)\n",
    "model3 = keras.Model(int_sequences_input, preds)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "523/523 [==============================] - 299s 570ms/step - loss: 0.6904 - acc: 0.7350 - val_loss: 1.7053 - val_acc: 0.4802\n",
      "Epoch 2/2\n",
      " 39/523 [=>............................] - ETA: 4:43 - loss: 0.1974 - acc: 0.9216"
     ]
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"]\n",
    ")\n",
    "\n",
    "history_3=model3.fit(x_train, y_train, batch_size=256, epochs=2, verbose=1, validation_data=(x_dev_full, y_dev))#validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 5s 58ms/step - loss: 2.7118 - acc: 0.4771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.7118325233459473, 0.4771241843700409]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(x_dev_full,y_dev,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1046/1046 [==============================] - 431s 412ms/step - loss: 0.1858 - acc: 0.9259 - val_loss: 2.2244 - val_acc: 0.4831\n",
      "Epoch 2/2\n",
      " 552/1046 [==============>...............] - ETA: 3:20 - loss: 0.1338 - acc: 0.9472"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-1b294a316c3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dev_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#validation_split=0.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model3.fit(x_train, y_train, batch_size=128, epochs=2, verbose=1, validation_data=(x_dev_full, y_dev))#validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "results=model3.predict(x_dev_full)\n",
    "predictions=np.argmax(results,axis=1)\n",
    "\n",
    "onehot_labels = enc_dev.get_feature_names()\n",
    "\n",
    "output_labels = list()\n",
    "for label in onehot_labels:\n",
    "    output_labels.append(label.replace(\"x0_\",\"\"))\n",
    "\n",
    "out = list()\n",
    "for prediction in predictions:\n",
    "    out.append(output_labels[prediction])\n",
    "\n",
    "predictions = enc.fit_transform(np.array(out).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.17      0.22      1484\n",
      "           1       0.51      0.66      0.58      4295\n",
      "           2       0.48      0.51      0.50      4266\n",
      "           3       0.31      0.16      0.21      1430\n",
      "\n",
      "   micro avg       0.48      0.48      0.48     11475\n",
      "   macro avg       0.41      0.37      0.38     11475\n",
      "weighted avg       0.45      0.48      0.46     11475\n",
      " samples avg       0.48      0.48      0.48     11475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_dev,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_3.history['acc'])\n",
    "plt.plot(history_3.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_3.history['loss'])\n",
    "plt.plot(history_3.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import numpy as np\n",
    "\n",
    "#model = ResNet50(weights='imagenet')\n",
    "plot_model(model3, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=model3.predict(x_test_full)\n",
    "predictions=np.argmax(results,axis=1)\n",
    "\n",
    "\n",
    "onehot_labels = enc_dev.get_feature_names()\n",
    "\n",
    "output_labels = list()\n",
    "for label in onehot_labels:\n",
    "    output_labels.append(label.replace(\"x0_\",\"\"))\n",
    "\n",
    "out = list()\n",
    "for prediction in predictions:\n",
    "    out.append(output_labels[prediction])\n",
    "\n",
    "# For output only:\n",
    "columns_titles = [\"id\",\"region\"]\n",
    "prediction=reformat_prediction(out)\n",
    "tweet_test_Y=prediction.reindex(columns=columns_titles)\n",
    "tweet_test_Y.id.astype(int)\n",
    "tweet_test_Y.to_csv('cnn_my_glove200d.csv',index=False,float_format='%.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My own Glove 300d training (not used)\n",
    "\n",
    "Use the same model as glove200d, but instead average them per tweet level, use embedded layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-70-f9413c76f9b1>:12: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  coefs = np.fromstring(coefs, sep=\" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1449265 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "path_to_my_glove_file = os.path.join(\"G:/UNIMELB/ML/A3/emb/glove.840B.300d.txt\")\n",
    "#G:/UNIMELB/ML/A3/emb/my_glove_vectors_b2.txt\n",
    "#G:/UNIMELB/ML/A3/emb/my_glove_vectors_b5.txt\n",
    "\n",
    "# Use my own training set as complement set\n",
    "embeddings_index = {}\n",
    "with open(path_to_my_glove_file,'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        word = word.translate(str.maketrans('', '', string.punctuation)).lower() #same way as keras Vectrization.\n",
    "        coefs = np.fromstring(coefs, sep=\" \")\n",
    "        if embeddings_index.get(word) is None: \n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 44331 words (98691 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "x=0\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    elif embeddings_index.get(\"#\"+word) is not None:\n",
    "        embedding_matrix[i] = embeddings_index.get(\"#\"+word)\n",
    "        hits += 1\n",
    "    elif embeddings_index.get(\"@\"+word) is not None:\n",
    "        embedding_matrix[i] = embeddings_index.get(\"@\"+word)\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model build and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-28c5b9106243>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mint_sequences_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"int64\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0membedded_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint_sequences_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_layer' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "f1 = layers.Conv1D(128, 2, activation=\"relu\")(embedded_sequences)\n",
    "f1 = layers.MaxPooling1D(5)(f1)\n",
    "f2 = layers.Conv1D(128, 3, activation=\"relu\")(embedded_sequences)\n",
    "f2 = layers.MaxPooling1D(5)(f2)\n",
    "f3 = layers.Conv1D(128, 4, activation=\"relu\")(embedded_sequences)\n",
    "f3 = layers.MaxPooling1D(5)(f3)\n",
    "x = layers.concatenate([f1,f2,f3])\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(labels), activation=\"softmax\")(x)\n",
    "model300d = keras.Model(int_sequences_input, preds)\n",
    "model300d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model300d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-9c6b7ca081ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model300d.compile(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"acc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel300d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model300d' is not defined"
     ]
    }
   ],
   "source": [
    "model300d.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"]\n",
    ")\n",
    "model300d.fit(x_train, y_train, batch_size=128, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model300d.evaluate(x_dev_full,y_dev,batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "f1 = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "f1 = layers.MaxPooling1D(5)(f1)\n",
    "f2 = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "f2 = layers.MaxPooling1D(5)(f2)\n",
    "f3 = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "f3 = layers.MaxPooling1D(5)(f3)\n",
    "x = layers.concatenate([f1,f2,f3])\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(labels), activation=\"softmax\")(x)\n",
    "model300d_worse = keras.Model(int_sequences_input, preds)\n",
    "model300d_worse.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model300d_worse.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"]\n",
    ")\n",
    "model300d_worse.fit(x_train, y_train, batch_size=128, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model300d_worse.evaluate(x_dev_full,y_dev,batch_size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: CHEATING DATA with CNN\n",
    "For my personal interest only, wont be used in the formal discussion, but here i concatenate both training and dev data as training set. See if this can bring the model to higher acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHEATING DATA\n",
    "x_cheat = np.concatenate((x_train_full,x_dev_full),axis=0)\n",
    "y_cheat = np.concatenate((y_train_full,y_dev),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My_glove embedded vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 200)    28604800    input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, None, 128)    51328       embedding_4[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, None, 128)    76928       embedding_4[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, None, 128)    102528      embedding_4[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, None, 128)    0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, None, 128)    0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, None, 128)    0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, None, 384)    0           max_pooling1d_30[0][0]           \n",
      "                                                                 max_pooling1d_31[0][0]           \n",
      "                                                                 max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 384)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 128)          49280       global_max_pooling1d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 4)            516         dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 28,885,380\n",
      "Trainable params: 28,885,380\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "f1 = layers.Conv1D(128, 2, activation=\"relu\")(embedded_sequences)\n",
    "f1 = layers.MaxPooling1D(5)(f1)\n",
    "f2 = layers.Conv1D(128, 3, activation=\"relu\")(embedded_sequences)\n",
    "f2 = layers.MaxPooling1D(5)(f2)\n",
    "f3 = layers.Conv1D(128, 4, activation=\"relu\")(embedded_sequences)\n",
    "f3 = layers.MaxPooling1D(5)(f3)\n",
    "x = layers.concatenate([f1,f2,f3])\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(labels), activation=\"softmax\")(x)\n",
    "model4 = keras.Model(int_sequences_input, preds)\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 960/4540 [=====>........................] - ETA: 17:37 - loss: 0.2460 - acc: 0.8990"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-659641f1a7eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"acc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m )\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_cheat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_cheat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model4.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"]\n",
    ")\n",
    "model4.fit(x_cheat, y_cheat, batch_size=32, epochs=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 5s 131ms/step - loss: 0.7933 - acc: 0.6597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7932902574539185, 0.6596949696540833]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate(x_dev_full,y_dev,batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=model4.predict(x_test_full)\n",
    "predictions=np.argmax(results,axis=1)\n",
    "\n",
    "\n",
    "onehot_labels = enc_dev.get_feature_names()\n",
    "\n",
    "output_labels = list()\n",
    "for label in onehot_labels:\n",
    "    output_labels.append(label.replace(\"x0_\",\"\"))\n",
    "\n",
    "out = list()\n",
    "for prediction in predictions:\n",
    "    out.append(output_labels[prediction])\n",
    "\n",
    "# For output only:\n",
    "columns_titles = [\"id\",\"region\"]\n",
    "prediction=reformat_prediction(out)\n",
    "tweet_test_Y=prediction.reindex(columns=columns_titles)\n",
    "tweet_test_Y.id.astype(int)\n",
    "tweet_test_Y.to_csv('cnn_cheat.csv',index=False,float_format='%.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHEATING DATA\n",
    "x_cheat = np.concatenate((tx,dx),axis=0)\n",
    "y_cheat = np.concatenate((ty,dy),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedded_sequences = keras.Input(shape=(300,1))\n",
    "\n",
    "f1 = layers.Conv1D(100, 5, activation=\"relu\")(embedded_sequences)\n",
    "f1 = layers.MaxPooling1D(5)(f1)\n",
    "f2 = layers.Conv1D(100, 5, activation=\"relu\")(embedded_sequences)\n",
    "f2 = layers.MaxPooling1D(5)(f2)\n",
    "f3 = layers.Conv1D(100, 5, activation=\"relu\")(embedded_sequences)\n",
    "f3 = layers.MaxPooling1D(5)(f3)\n",
    "x = layers.concatenate([f1,f2,f3])\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(labels), activation=\"softmax\")(x)\n",
    "model5 = keras.Model(embedded_sequences, preds)\n",
    "model5.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"]\n",
    ")\n",
    "model5.fit(x_cheat, y_cheat, batch_size=128, epochs=20, validation_data=(dx, dy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
